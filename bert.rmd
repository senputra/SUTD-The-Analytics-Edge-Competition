```{r}

library(dplyr)
library(tidyverse)
library(zeallot)
library(reticulate)
library(keras)
library(tensorflow)

Sys.setenv(TF_KERAS=1)
Sys.setenv(KERAS_BACKEND="plaidml.keras.backend")
use_python('/Users/dodysenputra/opt/anaconda3/envs/python/bin/python', required=T)
reticulate::py_config() # check if python is version >3

```



```{r}
py_module_available('keras_bert')
# tensorflow::install_tensorflow(version = "1.15")
```

```{r}
pretrained_path = '/Users/dodysenputra/Projects/kaggle/test'
config_path = file.path(pretrained_path, 'bert_config.json')
checkpoint_path = file.path(pretrained_path, 'bert_model.ckpt')
vocab_path = file.path(pretrained_path, 'vocab.txt')

k_bert = import('keras_bert')
token_dict = k_bert$load_vocabulary(vocab_path)
tokenizer = k_bert$Tokenizer(token_dict)
```

Prepare functions
```{r}
# tokenize text
tokenize_fun = function(dataset) {
  # print(dataset)
  c(indices, target, segments) %<-% list(list(), list(), list())
  for (i in 1:nrow(dataset)) {
    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i],
                                                       max_len = seq_length)
    indices = indices %>% append(list(as.matrix(indices_tok)))
    target = target %>% append(dataset[[LABEL_COLUMN]][i])
    segments = segments %>% append(list(as.matrix(segments_tok)))
  }
  return(list(indices, segments, target))
}
# read data
dt_data = function(dir, rows_to_read) {
  data = data.table::fread(dir, nrows = rows_to_read)
  c(x_train, x_segment, y_train) %<-% tokenize_fun(data)
  return(list(x_train, x_segment, y_train))
}



# tokenize text
tokenize_fun_test = function(dataset) {
  # print(dataset)
  c(indices, segments) %<-% list(list(), list())
  for (i in 1:nrow(dataset)) {
    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i],
                                                       max_len = seq_length)
    indices = indices %>% append(list(as.matrix(indices_tok)))
    segments = segments %>% append(list(as.matrix(segments_tok)))
  }
  return(list(indices, segments))
}
dt_data_test = function(dir, rows_to_read) {
  manualTest <- data.frame(tweet=c('hello i am great!', "fuk you"))
  # data = data.table::fread(dir, nrows = rows_to_read)
  c(x_train, x_segment) %<-% tokenize_fun_test(manualTest)
  return(list(x_train, x_segment))
}
```

```{r}
# 22500
c(x_train, x_segment, y_train) %<-%
  dt_data(file.path(pretrained_path, 'train.csv'), 2000)
```

some constants to load models
```{r}
seq_length = 50L
bch_size = 70
epochs = 1
learning_rate = 1e-4

DATA_COLUMN = 'tweet'
LABEL_COLUMN = 'sentiment'

model = k_bert$load_trained_model_from_checkpoint(
  config_path,
  checkpoint_path,
  training=T,
  trainable=T,
  seq_len=seq_length)
```





matrix format for keras-bert
```{r}
factorise <- function(data, ncol) {
  parsed = c()
  for (i in 1:length(data)) {
    x = data[i]
    parsed = c(parsed, c(
      if (x == 1) 1 else 0, 
      if (x == 2) 1 else 0, 
      if (x == 3) 1 else 0)
      )
  }
  return(matrix(parsed, ncol=ncol,byrow=TRUE))
}

train = do.call(cbind,x_train) %>% t()
segments = do.call(cbind,x_segment) %>% t()
targets = do.call(cbind,y_train) %>% t() %>% factorise(3)


```

```{r}
c(decay_steps, warmup_steps) %<-% k_bert$calc_train_steps(
  targets %>% length(),
  batch_size=bch_size,
  epochs=epochs
)
```


Determine inputs and outputs, then concatenate them
```{r}

input_1 = get_layer(model,name = 'Input-Token')$input
input_2 = get_layer(model,name = 'Input-Segment')$input
inputs = list(input_1,input_2)

dense = get_layer(model,name = 'NSP-Dense')$output

outputs = dense %>% layer_dense(units=3L,
                                activation='sigmoid',
                         kernel_initializer=initializer_truncated_normal(stddev = 0.02),
                         name = 'output')

model = keras_model(inputs = inputs,outputs = outputs)
```


fit()

```{r}
model %>% compile(
  k_bert$AdamWarmup(decay_steps=decay_steps, 
                    warmup_steps=warmup_steps, lr=learning_rate),
  loss = 'binary_crossentropy',
  metrics = 'accuracy'
)

model %>% fit(
  concat,
  parsed_targets,
  epochs = epochs,
  batch_size = bch_size,
  validation_split = 0.2
)

```

```{r}

c(x_test, x_segment) %<-%
  dt_data_test(file.path(pretrained_path, 'test.csv'), 2000)
test = do.call(cbind,x_test) %>% t()
segments = do.call(cbind,x_segment) %>% t()

concat = c(list(train),list(segments))

prediction <- model %>% predict(concat)
prediction
```